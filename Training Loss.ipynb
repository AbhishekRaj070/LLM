{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac63f6d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Input Processing:</b> Reading the book called \"The Verdict\" into the Notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cedd42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with open('the-verdict.txt', mode='r',encoding='utf-8') as data:\n",
    "    text_data = data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a5b50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Encoding Class:</b> Encoding using Tiktoken tokenizer. Using GPT2 encoding model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58ea1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self,text_data):\n",
    "        import tiktoken\n",
    "        self._text_data = text_data\n",
    "        self._encoder = tiktoken.get_encoding('gpt2')\n",
    "        \n",
    "    def Encoding(self):\n",
    "        ids = self._encoder.encode(self._text_data)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64585667",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tokenizer Example:</b> Using artificial data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa72398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 655, 281, 1672, 4731, 13]\n"
     ]
    }
   ],
   "source": [
    "Id = Tokenizer('This is just an example string.').Encoding()\n",
    "print(Id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705b508",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Dataset Class:</b> Dataset class is created using dataset and dataloader class from Torch.utils.data  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da8a9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset_(Dataset):\n",
    "    # Takes a list of ids to initialize\n",
    "    def __init__(self, ids, context_length, stride):\n",
    "        \n",
    "        self._ids = ids\n",
    "        self._Inputs = []\n",
    "        self._Targets = []\n",
    "        \n",
    "    # Creates input matrix consisting of ids arranged in batches * context_lenght\n",
    "    # Also creates output for each input \n",
    "        \n",
    "        for i in range(0, len(self._ids)-context_length, stride):\n",
    "            \n",
    "            # take the ids from ith position to all the way to the conext size\n",
    "            input_ids = self._ids[i:i+context_length]\n",
    "            target_ids = self._ids[i+1:i+context_length+1]\n",
    "            \n",
    "            # append the temporary inputs and outputs to main Input and Output array object\n",
    "            self._Inputs.append(torch.tensor(input_ids))\n",
    "            self._Targets.append(torch.tensor(target_ids))\n",
    " \n",
    "    #def get(self, rowx):\n",
    "        #return self._Inputs[rowx], self._Targets[rowx] \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self._Inputs)\n",
    "    \n",
    "    # Returns the input and outpair of desired row\n",
    "    def __getitem__(self, rowx):\n",
    "        \n",
    "        return self._Inputs[rowx], self._Targets[rowx] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427295e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Dataloader Function:</b> Dataset for training and testing \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70d30d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(text, batch_size = 4,\n",
    "              context_length = 256 ,\n",
    "              stride= 4,\n",
    "              drop_last = True,\n",
    "              shuffle = False,\n",
    "              num_workers = 0):\n",
    "    \n",
    "    ids = Tokenizer(text).Encoding()\n",
    "    \n",
    "    dataset = Dataset_(ids,\n",
    "                       context_length,\n",
    "                       stride)\n",
    "    \n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size,\n",
    "                            shuffle = False,\n",
    "                            drop_last = True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81df7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_config = {\"Vocab_size\": 50275,    # Total Byte-Pair encodding dictionary size\n",
    "            \"Context_lenght\": 256,   # Length of Input context at a time\n",
    "            \"embedding_dim\": 768,     # Total embedding dimension \n",
    "            \"Number_of_heads\": 12,    # Total number of heads in Multihead Attention Mechanis\n",
    "            \"Number_of_layers\": 12,   # Toral number of transformer blocks\n",
    "            \"Dropout_rate\": 0.1,      # Neuron drop out rate\n",
    "            \"QKV_bias\": False         # Query, Key and Value bias\n",
    "             }\n",
    "\n",
    "## train and testing data split\n",
    "\n",
    "train_test_split = 0.90\n",
    "index = int(train_test_split * len(text_data))\n",
    "train_text = text_data[:index]\n",
    "test_text = text_data[index:]\n",
    "\n",
    "\n",
    "## test - train data loader\n",
    "## set of batches\n",
    "train_dataloader = Load_Data(train_text,\n",
    "                            batch_size=2,\n",
    "                            context_length=GPT_config['Context_lenght'],\n",
    "                            stride = GPT_config['Context_lenght'],\n",
    "                            drop_last= True)\n",
    "\n",
    "test_dataloader = Load_Data(test_data,\n",
    "                           batch_size=2,\n",
    "                           context_length=GPT_config['Context_lenght'],\n",
    "                           stride = GPT_config['Context_lenght'],\n",
    "                           drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4a8f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Testing tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "## Checking number of tokens in each loaders\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "test_tokens = 0\n",
    "for input_batch, target_batch in test_dataloader:\n",
    "    test_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Testing tokens:\", test_tokens)\n",
    "print(\"All tokens:\", train_tokens + test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f2c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Normalization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._epsilon = 1e-5 # to prevent zero division error\n",
    "        self._scaling = torch.nn.Parameter(torch.ones(emb_dim)) \n",
    "        self._shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        mean = torch.mean(X, dim=-1, keepdim =True)\n",
    "        var = torch.var(X, dim=-1, keepdim=True, unbiased = False)\n",
    "        Normalized_X = (X - mean)/torch.sqrt(var + self._epsilon)\n",
    "        \n",
    "        return Normalized_X*self._scaling + self._shift\n",
    "    \n",
    "    \n",
    "class GELU_Activation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))  \n",
    "    \n",
    "class Feed_Forward(torch.nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self._Model = torch.nn.Sequential(torch.nn.Linear(emb_dim, 4*emb_dim), GELU_Activation(),\n",
    "                            torch.nn.Linear(4*emb_dim, emb_dim)\n",
    "                           )\n",
    "    def forward(self,X):\n",
    "        return self._Model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73cf27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_in, dim_out, num_heads, dropout): # let just assume what out dimension we want\n",
    "        super().__init__()\n",
    "        self._dim_out = dim_out\n",
    "        self._QueryW = torch.nn.Linear(dim_in, dim_out,bias = False) # Initialize the weights for Query, Key and Value weights\n",
    "        self._KeyW = torch.nn.Linear(dim_in, dim_out,bias= False)\n",
    "        self._ValueW = torch.nn.Linear(dim_in, dim_out,bias = False)\n",
    "        \n",
    "        self._Dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize number of heads\n",
    "        self._heads = num_heads\n",
    "        # Remainder that to be zero, equal spitting of heads into dimensions\n",
    "        assert (dim_out%self._heads == 0), \"Has to be divisible\"\n",
    "        self._head_dimensions = dim_out//num_heads\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,X):  \n",
    "        # X.shape[0] = batch_size, X.shape[1] = tot_embedding, X.shape[2] = dim_in\n",
    "        batch_size, tot_embedding, dim_in = X.shape\n",
    "        # calculation for Query, Key and Value matrix\n",
    "        Query = self._QueryW(X)\n",
    "        Key = self._KeyW(X)\n",
    "        Value = self._ValueW(X)\n",
    "        \n",
    "        # The transition to split it in number of heads and dimensions\n",
    "        Query = Query.view(batch_size, tot_embedding, self._heads, self._head_dimensions)\n",
    "        Key = Key.view(batch_size, tot_embedding, self._heads, self._head_dimensions)\n",
    "        Value = Value.view(batch_size, tot_embedding, self._heads, self._head_dimensions)\n",
    "        \n",
    "        # Transpose the position of tot_embedding with total number of heads\n",
    "        Query = Query.transpose(1,2)\n",
    "        Key = Key.transpose(1,2)\n",
    "        Value = Value.transpose(1,2)\n",
    "        \n",
    "        attention_score = Query @ Key.transpose(2,3)\n",
    "        \n",
    "        # Initialization of mask\n",
    "        mask = torch.tril(torch.ones(tot_embedding,tot_embedding))\n",
    "        \n",
    "        # Normalization of attention_score\n",
    "        attention_score_normalized = torch.softmax(attention_score/Key.shape[-1]**0.5, dim=-1) \n",
    "        \n",
    "        # Masking the attention_score\n",
    "        attention_score_masked = attention_score_normalized * mask\n",
    "        \n",
    "        # Normalization to 1\n",
    "        attention_weights = attention_score_masked/torch.sum(attention_score_masked, dim =-1, keepdim = True)\n",
    "        \n",
    "        attention_weights = self._Dropout(attention_weights)\n",
    "        \n",
    "        # Context_vector calculation\n",
    "        ### Transpose the context vector to rearrange and get number of heads close to head_dimensions\n",
    "        context_vector = (attention_weights @ Value).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        context_vector = context_vector.contiguous().view(batch_size, tot_embedding, self._dim_out)\n",
    "        \n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "class Transformer_Block(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, GPT_config):\n",
    "        super().__init__()\n",
    "        ##### Takes tensor in forward method ##### Test with tensor\n",
    "        self._Multihead_Attention = Multihead_Attention(GPT_config[\"embedding_dim\"], # dim_in = dim_out\n",
    "                                                        GPT_config[\"embedding_dim\"],\n",
    "                                                        GPT_config[\"Number_of_heads\"],\n",
    "                                                        GPT_config[\"Dropout_rate\"])\n",
    "        \n",
    "        self._Layer_Normalization1 = Layer_Normalization(GPT_config[\"embedding_dim\"]) # Linear Normalization Layer\n",
    "        self._Layer_Normalization2 = Layer_Normalization(GPT_config[\"embedding_dim\"])\n",
    "        \n",
    "        self._Feed_Forward = Feed_Forward(GPT_config[\"embedding_dim\"]) # Expansion and contraction layer\n",
    "        \n",
    "        self._Dropout_Short = torch.nn.Dropout(GPT_config[\"Dropout_rate\"]) # drop out step\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        shortcut = X                                # seperate line for the short cut\n",
    "        \n",
    "        X = self._Layer_Normalization1(X)           # First Layer Normalization\n",
    "        \n",
    "        X = self._Multihead_Attention(X)             # Pass through Multihead Attention\n",
    "        \n",
    "        X = self._Dropout_Short(X)                  # Drop some Neurons\n",
    "        \n",
    "        X = X + shortcut                             # Merge with the shortcut from the input\n",
    "        \n",
    "        ## Second stage of transformer \n",
    "        \n",
    "        shortcut = X\n",
    "        \n",
    "        X = self._Layer_Normalization2(X)\n",
    "        \n",
    "        X = self._Feed_Forward(X)\n",
    "        \n",
    "        X = self._Dropout_Short(X)\n",
    "        \n",
    "        X = X + shortcut\n",
    "        \n",
    "        return X       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4c45c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Model(torch.nn.Module):\n",
    "    def __init__(self, GPT_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._Embeddings = torch.nn.Embedding(GPT_config['Vocab_size'],\n",
    "                                             GPT_config['embedding_dim'])\n",
    "        self._Positional_Embedding = torch.nn.Embedding(GPT_config['Context_lenght'],\n",
    "                                             GPT_config['embedding_dim'])\n",
    "        self._Dropout = torch.nn.Dropout(GPT_config['Dropout_rate'])\n",
    "        \n",
    "        # Transformer step\n",
    "        self._Transformer = torch.nn.Sequential(\n",
    "                                        *[Transformer_Block(GPT_config) for _ in range(GPT_config['Number_of_layers'])])\n",
    "        \n",
    "        # Output steps\n",
    "        self._LayerNormalization = Layer_Normalization(GPT_config['embedding_dim'])\n",
    "        self._Out = torch.nn.Linear(GPT_config['embedding_dim'], GPT_config['Vocab_size'], bias = False)\n",
    "        \n",
    "    def forward(self, ids):                         # Here X is input of dimension Batch size and context lenght\n",
    "        \n",
    "        batch_size, context_len = ids.shape\n",
    "        \n",
    "        token_embedding = self._Embeddings(ids)     # X become 3 dimensional tensor Batch size, context length and embedding dim \n",
    "        positional_embedding = self._Positional_Embedding(torch.arange(context_len, device = ids.device))\n",
    "        \n",
    "        X = token_embedding + positional_embedding # Add the positional embedding\n",
    "        \n",
    "        X = self._Dropout(X)                       # Drop out \n",
    "        X = self._Transformer(X)                   # Pass through the transformer             \n",
    "        \n",
    "        X = self._LayerNormalization(X)            # layer Normalization, mean = 0, standard deviation = 1\n",
    "        logits = self._Out(X)\n",
    "        \n",
    "        return logits \n",
    "    \n",
    "## Have to perform softmax on the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff1130a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss calculation for one batch\n",
    "def Cal_loss_batch(Input_batch, Target_batch, model, device):\n",
    "    \n",
    "    Input_batch, Target_Batch = Input_batch.to(device), Target_batch.to(device)\n",
    "    logits = model(Input_batch)\n",
    "    ## flaten the logits to remove the batch dimensions and club all the context together\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), Target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "## Loss calculation for loader\n",
    "\n",
    "def Cal_loss_loader(dataloader, model, device, num_batches =None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return \"Nothing in the loader\"\n",
    "    \n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "    else:\n",
    "        num_batches = min(len(dataloader), num_batches)\n",
    "        \n",
    "    for i, (Input, Target) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = Cal_loss_batch(Input, Target, model, device)\n",
    "            total_loss = loss.item() + total_loss\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96f853cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPT Model of GPT Model class\n",
    "model = GPT_Model(GPT_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95514e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is --10.977535247802734\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    training_loss = Cal_loss_loader(train_dataloader, model, device)\n",
    "    validation_loss = Cal_loss_loader(test_dataloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48bc3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9775)\n",
      "tensor(11.0123)\n"
     ]
    }
   ],
   "source": [
    "print(training_loss)\n",
    "print(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae33a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
